{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f1fd19-f818-4b15-8af0-a1e46f4b9824",
   "metadata": {},
   "source": [
    "# Lecture 15: Class demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50586ed0-b8e0-4725-be27-b64db91ee929",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's cluster images!!\n",
    "\n",
    "![](../../img/eva-fun-times.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d02dc-f0fa-42ce-b1d2-e5f0bb551dae",
   "metadata": {},
   "source": [
    "For this demo, I'm going to use the following image dataset: \n",
    "1. A tiny subset of [Food-101](https://www.kaggle.com/datasets/kmader/food41?select=food_c101_n10099_r32x32x1.h5) from last lecture\n",
    "(available [here](https://github.ubc.ca/mds-2021-22/datasets/blob/master/data/food.zip)).\n",
    "2. A small subset of [Human Faces dataset](https://www.kaggle.com/datasets/ashwingupta3012/human-faces) (available [here](https://ubcca-my.sharepoint.com/:u:/g/personal/varada_kolhatkar_ubc_ca/EYDqm7QJLfdGh1A0dyqh76kB6PH9ohca-lVrJGATrEh3CQ?e=msqcPM))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6bac4c-3ece-408a-b907-f39a5762fda1",
   "metadata": {},
   "source": [
    "To run the code below, you need to install pytorch and torchvision in the course conda environment. \n",
    "\n",
    "```conda install pytorch torchvision -c pytorch```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dec135-7eab-4148-be95-a056699b0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.join(\"code\"))\n",
    "from plotting_functions_unsup import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c2a17-7441-4046-9a4e-c598801eea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697cee92-1592-4d8c-83c8-36de4381248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d1361-c41e-4938-aab9-fc80840e6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee4e5e-7c3e-424d-9f78-44be75e32cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "IMAGE_SIZE = 224\n",
    "def read_img_dataset(data_dir):     \n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),     \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),            \n",
    "        ])\n",
    "               \n",
    "    image_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "         image_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    "    )\n",
    "    dataset_size = len(image_dataset)\n",
    "    class_names = image_dataset.classes\n",
    "    inputs, classes = next(iter(dataloader))\n",
    "    return inputs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28a0e1-7db8-4525-b57e-35adc2fd7e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_imgs(inputs):\n",
    "    plt.figure(figsize=(10, 70)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\n",
    "    plt.imshow(np.transpose(utils.make_grid(inputs, padding=1, normalize=True),(1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4433789-1f1d-4fa8-8001-39d608c77e81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(model, inputs):\n",
    "    \"\"\"Extract output of densenet model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # turn off computational graph stuff        \n",
    "        Z = model(inputs).detach().numpy()         \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6d5a8-426d-4fc6-adcc-cc321ce5ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\n",
    "densenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057097fa-349f-441e-8914-40290d09b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/food\"\n",
    "file_names = [image_file for image_file in glob.glob(data_dir + \"/*/*.jpg\")]\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "food_inputs, food_classes = read_img_dataset(data_dir)\n",
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4199e-17a3-41e8-9437-7ef4b5f0a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_food = food_inputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd6ccd-98d3-404d-acfb-512ee0c6949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_imgs(food_inputs[0:24,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10694b03-73d7-4420-bf55-663a9a189feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_food = get_features(\n",
    "    densenet, food_inputs, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a001e-66fb-44da-9d71-7736a6e4d09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28bdcd-e3b9-4cd8-9b10-066411df6add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "km = KMeans(n_clusters=k, n_init='auto', random_state=123)\n",
    "km.fit(Z_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e56660-d707-478f-a94a-709dc8c9fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f360a2a2-22f5-4ef7-91af-797c1f80f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    get_cluster_images(km, Z_food, X_food, cluster, n_img=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9c8c6-a8b4-4a1c-8b89-ce8be0097c35",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928527b-a2fe-4af0-96d4-3b41f66c9d25",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7043b4c-88ec-4720-8578-69b5c357a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN()\n",
    "\n",
    "labels = dbscan.fit_predict(Z_food)\n",
    "print(\"Unique labels: {}\".format(np.unique(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3284a1fa-4862-455a-8bf4-4a162a96dba6",
   "metadata": {},
   "source": [
    "It identified all points as noise points. Let's explore the distances between points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784446d1-2d50-4e02-9d4f-df603d57e544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "dists = euclidean_distances(Z_food)\n",
    "np.fill_diagonal(dists, np.inf)\n",
    "dists_df = pd.DataFrame(dists)\n",
    "dists_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36022d1f-7e8a-4f1b-b4a9-9074780990b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists.min(), np.nanmax(dists[dists != np.inf]), np.mean(dists[dists != np.inf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5798149-3ade-450e-91a5-1eca8c639c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for eps in range(13, 20):\n",
    "    print(\"\\neps={}\".format(eps))\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=3)\n",
    "    labels = dbscan.fit_predict(Z_food)\n",
    "    print(\"Number of clusters: {}\".format(len(np.unique(labels))))\n",
    "    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45460486-f4da-42c9-a680-5c3821a621be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=14, min_samples=3)\n",
    "dbscan_labels = dbscan.fit_predict(Z_food)\n",
    "print(\"Number of clusters: {}\".format(len(np.unique(dbscan_labels))))\n",
    "print(\"Cluster sizes: {}\".format(np.bincount(dbscan_labels + 1)))\n",
    "print(\"Unique labels: {}\".format(np.unique(dbscan_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f61d29-8357-410d-bf36-d0abed500580",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dbscan_clusters(Z_food, food_inputs, dbscan_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de59b0-18e6-4567-ab4a-4d84adf76963",
   "metadata": {},
   "source": [
    "Let's examine noise points identified by DBSCAN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089c766-e8be-4159-ac52-9a89984a47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dbscan_noise_images(Z_food, food_inputs, dbscan_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48527d88-4ac4-4597-af17-0097467ea532",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8971fbe-5e58-4b66-82f5-5321f10a186d",
   "metadata": {},
   "source": [
    "### Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c205fe-1bf5-45c3-8586-78cb588cefa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c719184-f348-499b-bc7d-62460366b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "Z_hrch = ward(Z_food)\n",
    "dendrogram(Z_hrch, p=7, truncate_mode=\"level\", no_labels=True)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdda6a-d3a0-464c-9b7d-27080b27c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = fcluster(Z_hrch, 20, criterion=\"maxclust\")  # let's get flat clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc0a44-e92d-4ce6-b99c-43b62b0f68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_picked_clusters = np.arange(2, 20)\n",
    "#hand_picked_clusters = [2, 3, 5, 6,7, 8, 9, 10, 12, 14,15,16,17,19,20, 21,22, 24, 26, 27, 28]\n",
    "print_hierarchical_clusters(\n",
    "    food_inputs, Z_food, cluster_labels, hand_picked_clusters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c212f-c043-4631-84d5-13c7385f01ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Some clusters correspond to people with distinct faces, age, facial expressions, hair colour and hair style, lighting and skin tone. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
